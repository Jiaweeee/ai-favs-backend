<html id="html-top" lang="zh">
 <head>
  <meta charset="UTF-8">
  <title>GitHub热榜第一：百万token上下文，还能生成视频，UC伯克利出品</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <link rel="icon" href="https://cubox.pro/my/favicon.svg" type="image/x-icon">
  <link href="https://cubox.pro/article/css/reader.css" rel="stylesheet">
 </head>
 <body ontouchstart>
  <div class="reader-page">
   <div>
    <h1 class="reader-title">GitHub热榜第一：百万token上下文，还能生成视频，UC伯克利出品</h1>
    <div class="reader-metadata">
     <a href="https://m.toutiao.com/article/7337207636644856332/?app=news_article&amp;timestamp=1708338634&amp;use_new_style=1&amp;req_id=2024021918303391870E470948640E7FC9&amp;group_id=7337207636644856332&amp;wxshare_count=1&amp;tt_from=weixin&amp;utm_source=weixin&amp;utm_medium=toutiao_android&amp;utm_campaign=client_share&amp;share_token=9cfbb832-549f-429b-ba35-fde1deb555a7" target="_blank">m.toutiao.com</a><span class="reader-metadata-author">量子位7个小时前 · 量子位官方账号 优质科技领域创作者关注</span>
    </div>
    <article> 
     <blockquote> 
      <p data-track="42">克雷西 发自 凹非寺</p> 
      <p data-track="43">量子位 | 公众号 QbitAI</p> 
     </blockquote> 
     <p data-track="2">今日GitHub热榜榜首，是最新的开源世界模型。</p> 
     <p data-track="3">上下文窗口长度达到了100万token，持平了谷歌同时推出的王炸Gemini 1.5，伯克利出品。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-axegupay5k/8d0d859806d6405c8704601be60fc46d~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=RNYRc316NvsVm3iUwvtkSVwD5pQ%3D" src="https://image.cubox.pro/cardImg/2024021922503270300/37458.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-axegupay5k/8d0d859806d6405c8704601be60fc46d~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=RNYRc316NvsVm3iUwvtkSVwD5pQ%3D"></p> 
     </div> 
     <p data-track="4">强大的模型，命名也是简单粗暴——没有任何额外点缀，直接就叫LargeWorldModel（LWM）。</p> 
     <p data-track="5">LWM支持处理多模态信息，能在100万token中准确找到目标文本，还能一口气看完1小时的视频。</p> 
     <p data-track="6">网友看了不禁表示，这种大海捞针般的测试，LWM能完成的如此出色，而且还开源，实在是令人印象深刻。</p> 
     <div> 
      <p><img data-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/0810e414366742528799c27a1c2eb2de~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=obDgHn%2Bw%2F8H%2Fm3jGwUBd%2F4TZXyc%3D" src="https://image.cubox.pro/cardImg/2024021922503380480/22398.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/0810e414366742528799c27a1c2eb2de~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=obDgHn%2Bw%2F8H%2Fm3jGwUBd%2F4TZXyc%3D"></p> 
     </div> 
     <p data-track="7">那么，LWM的表现到底有多强呢？</p> 
     <h2 data-track="8">百万上下文窗口，可看1小时视频</h2> 
     <p data-track="9">在测试过程中，研究人员用多段一个多小时的视频检验了LWM的长序列理解能力，这些视频由YouTube上不同的视频片段拼接而成。</p> 
     <p data-track="10">他们将这些视频输入LWM，然后针对其中的细节进行提问，涉及的片段位于整个视频的不同位置，同时研究者还将LWM与GPT-4V等模型做了对比。</p> 
     <p data-track="11">结果GPT-4V是一问一个不吱声，闭源强者Gemini Pro和开源强者Video-LLaVA都给出了错误的答案，只有LWM回答对了。</p> 
     <div> 
      <p><img data-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/607d6a3f42c6492f8c07ae6ea8e5615a~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=buL5bvjA%2FMwXGq8by3oMO90mywA%3D" src="https://image.cubox.pro/cardImg/2024021922503321572/88472.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/607d6a3f42c6492f8c07ae6ea8e5615a~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=buL5bvjA%2FMwXGq8by3oMO90mywA%3D"></p> 
     </div> 
     <p data-track="12">在另一段视频的测试中，其他模型都说找不到有关信息，只有LWM找到了答案，而且完全正确。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/101934b775ec4e21a22fac48dcbe4e00~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=rC8g0m4fPGU4lIcyH%2Boan%2Bq918s%3D" src="https://image.cubox.pro/cardImg/2024021922503325550/99285.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/101934b775ec4e21a22fac48dcbe4e00~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=rC8g0m4fPGU4lIcyH%2Boan%2Bq918s%3D"></p> 
     </div> 
     <p data-track="13">不仅是理解细节，LWM也能把握视频的整体内容，做出归纳总结。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/0970c99557c54034af41ee75b7986c8c~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=d8C7xyWGGXNTe%2FYy1%2Bqi%2BMKv5Cs%3D" src="https://image.cubox.pro/cardImg/2024021922503354930/53741.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/0970c99557c54034af41ee75b7986c8c~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=d8C7xyWGGXNTe%2FYy1%2Bqi%2BMKv5Cs%3D"></p> 
     </div> 
     <p data-track="14">在理解的基础之上，LWM也可以结合自有知识进行推理，比如分析视频中不符合常理的地方。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/9f4ca3bb60e94b31a1b5ea4563795611~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=XyjWQzzS8IUGFveuIylu5BGMqXM%3D" src="https://image.cubox.pro/cardImg/2024021922503445619/43707.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/9f4ca3bb60e94b31a1b5ea4563795611~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=XyjWQzzS8IUGFveuIylu5BGMqXM%3D"></p> 
     </div> 
     <p data-track="15">Benchmark测试结果显示，LWM在MSVD-QA等三个数据集上的评分仅次于Video-LLaVA。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5ffcfd01030e4a4aa3e01521a2fa27fd~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=QS5r%2BIW%2BO42vHenlCILukQeLYm8%3D" src="https://image.cubox.pro/cardImg/2024021922503412026/21109.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5ffcfd01030e4a4aa3e01521a2fa27fd~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=QS5r%2BIW%2BO42vHenlCILukQeLYm8%3D"></p> 
     </div> 
     <p data-track="16">LWM不仅能理解长短视频，在超长文本任务上的表现同样优异。</p> 
     <p data-track="17">在1百万token窗口的“插针”检索测试中，LWM取得了单针检索全绿的成绩。</p> 
     <div> 
      <p><img data-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/d166da2911124cd9add9fcb6b0c904a4~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=hinixLz3IuozGN%2FEPsHr%2B5fwT2c%3D" src="https://image.cubox.pro/cardImg/2024021922503447150/18817.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p9-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/d166da2911124cd9add9fcb6b0c904a4~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=hinixLz3IuozGN%2FEPsHr%2B5fwT2c%3D"></p> 
     </div> 
     <p data-track="18">多针检索时，表现也同样优异：</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/a5c2598ef73643bb99e5ef91c8152e63~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=h%2Feq7Sxeme%2B%2Fv9hx041amjFJH08%3D" src="https://image.cubox.pro/cardImg/2024021922503478902/51860.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/a5c2598ef73643bb99e5ef91c8152e63~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=h%2Feq7Sxeme%2B%2Fv9hx041amjFJH08%3D"></p> 
     </div> 
     <p data-track="19">语言任务数据集的测试结果表明，LWM在32k到1M的窗口长度上表现不输甚至超过只有4k窗口的Llama2-7B。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5d23b6ee0cb04405b2f013fb8a8a55c1~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=uRlbHvTJDw9nsZ%2B4UBbOLtiO%2B%2Bk%3D" src="https://image.cubox.pro/cardImg/2024021922503467736/20134.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5d23b6ee0cb04405b2f013fb8a8a55c1~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=uRlbHvTJDw9nsZ%2B4UBbOLtiO%2B%2Bk%3D"></p> 
     </div> 
     <p data-track="20">除了多模态信息理解，LWM还支持图像和视频的生成，至于效果，还是直接上图感受一下吧。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/d8fbf1554f604773a9f3c57efdfcec3c~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=Vcu2idw%2Bp0XmwmaJPZPIqPenmiA%3D" src="https://image.cubox.pro/cardImg/2024021922503514499/50399.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/d8fbf1554f604773a9f3c57efdfcec3c~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=Vcu2idw%2Bp0XmwmaJPZPIqPenmiA%3D"></p> 
     </div> 
     <p data-track="21">那么，研究人员又是怎样训练出这样一款世界模型的呢？</p> 
     <h2 data-track="22">循序渐进，分而治之</h2> 
     <p data-track="23">LMW的训练过程，大致可分为两个阶段。</p> 
     <p data-track="24"><strong><span>第一阶段</span></strong>的目标是建立一个能够处理长文本序列的语言模型，以理解复杂的文档和长文本内容。</p> 
     <p data-track="25">为实现这一目的，研究人员采取了渐进式的训练方式，使用总计33B Token、由图书内容组成的Books3数据集，从32k开始训练，逐步将窗口扩增至1M。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/1da01eb5875840688806e3dee552edc6~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=%2FDHVxJxHBAf1dTOKGyPNbdjQmcE%3D" src="https://image.cubox.pro/cardImg/2024021922503540307/61926.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/1da01eb5875840688806e3dee552edc6~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=%2FDHVxJxHBAf1dTOKGyPNbdjQmcE%3D"></p> 
     </div> 
     <p data-track="26">而为了增强LWM的长文本处理能力，开发者应用了RingAttention机制。</p> 
     <p data-track="27">RingAttention是该团队去年提出的一种窗口扩增方式，入选了ICLR 2024。</p> 
     <p data-track="28">它运用了“分而治之”的思想，将长文本分成多个块，用多个计算设备做序列并行处理，然后再进行叠加，理论上允许模型扩展到无限长的上下文。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/c871d1139bcc4429957b6e1f48b69886~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=nIsJaMxVB1UvXTTeNHocEddih9I%3D" src="https://image.cubox.pro/cardImg/2024021922503579899/34022.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/c871d1139bcc4429957b6e1f48b69886~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=nIsJaMxVB1UvXTTeNHocEddih9I%3D"></p> 
     </div> 
     <p data-track="29">在LWM中，RingAttention还与FlashAttention结合使用，并通过Pallas框架进行优化，从而提高性能。</p> 
     <p data-track="30">在文本能力的基础上，研究人员又用模型生成了部分QA数据，针对LWM的对话能力进行了优化。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/8f896e641ae24041a60ac3e440befcd7~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=ie8eeBPlV4mLGEwBg6Zfzh5X94A%3D" src="https://image.cubox.pro/cardImg/2024021922503561940/72284.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/8f896e641ae24041a60ac3e440befcd7~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=ie8eeBPlV4mLGEwBg6Zfzh5X94A%3D"></p> 
     </div> 
     <p data-track="31"><strong><span>第二阶段</span></strong>则是将视觉信息（如图像和视频）整合到模型中，以提高对多模态数据的理解能力。</p> 
     <p data-track="32">在此阶段，研究人员对LWM-Text模型进行了架构修改，以支持视觉输入。</p> 
     <p data-track="33">他们使用VQGAN将图像和视频帧转换为token，并与文本结合进行训练。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/8bf624f1573b4db2bd749bc4c516f524~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=XRQ%2F0sVDKl%2FZURG%2FhnU0AdQogpM%3D" src="https://image.cubox.pro/cardImg/2024021922503680981/92755.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/8bf624f1573b4db2bd749bc4c516f524~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=XRQ%2F0sVDKl%2FZURG%2FhnU0AdQogpM%3D"></p> 
     </div> 
     <p data-track="34">这一阶段同样采用循序渐进的训练方法， LWM首先在文本-图像数据集上进行训练，然后扩展到文本-视频数据集，且视频帧数逐步增多。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5dead4aa368f44dfb79f34edc1772579~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=7AcW9o84GmERnKxlGaSqCGH4zVc%3D" src="https://image.cubox.pro/cardImg/2024021922503635981/43029.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/5dead4aa368f44dfb79f34edc1772579~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=7AcW9o84GmERnKxlGaSqCGH4zVc%3D"></p> 
     </div> 
     <p data-track="35">在训练过程中，模型还会随机交换文本和视觉数据的顺序，以学习文本-图像生成、图像理解、文本-视频生成和视频理解等多种任务。</p> 
     <p data-track="36">性能方面，研究人员在TPUv4-1024（大致相对于450块A100）上训练，批大小为8M、全精度（float32）的条件下，花费的时间如下表所示，其中1M窗口版本用了58个小时。</p> 
     <div> 
      <p><img data-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/48d0ee821c9f443292a08f6d17e0833e~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=5WUrZkNOaEWcSMv3tjGSgAAfDcY%3D" src="https://image.cubox.pro/cardImg/2024021922503645094/67485.jpg?imageMogr2/quality/90/ignore-error/1" loading="lazy" origin-src="https://p3-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/48d0ee821c9f443292a08f6d17e0833e~tplv-tt-large.image?_iz=30575&amp;lk3s=06827d14&amp;x-expires=1708959028&amp;x-signature=5WUrZkNOaEWcSMv3tjGSgAAfDcY%3D"></p> 
     </div> 
     <p data-track="37">目前，LWM的代码、模型都已开源，其中多模态模型为Jax版本，纯文本模型有Jax和PyTorch两个版本，感兴趣的话可以到GitHub页面中了解详情。</p> 
     <p data-track="38"><span>论文地址：<br>https://arxiv.org/abs/2402.08268</span><br><span>GitHub：<br>https://github.com/LargeWorldModel/LWM</span></p> 
     <p data-track="39">— 完 —</p> 
     <p data-track="40">量子位 QbitAI · 头条号签约</p> 
     <p data-track="41">关注我们，第一时间获知前沿科技动态</p> 
    </article>
    <p class="reader-footer"><a class="reader-footer-source" href="https://cubox.pro/my/card?id=7159228443157070554" target="_blank"><span class="reader-footer-source-label">跳转到 Cubox 查看</span></a></p>
   </div>
  </div>
  <script type="text/javascript" src="https://cubox.pro/article/js/reader.js"></script>
 </body>
</html>