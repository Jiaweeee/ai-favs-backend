<html id="html-top" lang="zh">
 <head>
  <meta charset="UTF-8">
  <title>Fine-Tuning LLaMA 2: A Step-by-Step Guide to Customizing the Large Language Model</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <link rel="icon" href="https://cubox.pro/my/favicon.svg" type="image/x-icon">
  <link href="https://cubox.pro/article/css/reader.css" rel="stylesheet">
 </head>
 <body ontouchstart>
  <div class="reader-page">
   <div>
    <h1 class="reader-title">Fine-Tuning LLaMA 2: A Step-by-Step Guide to Customizing the Large Language Model</h1>
    <div class="reader-metadata">
     <a href="https://www.datacamp.com/tutorial/fine-tuning-llama-2" target="_blank">www.datacamp.com</a><span class="reader-metadata-author">Abid Ali Awan</span>
    </div>
    <div> 
     <p><span></span></p> 
     <div> 
      <table> 
       <colgroup> 
        <col> 
       </colgroup> 
       <tbody> 
        <tr> 
         <td>Read the <a rel="noopener" href="http://www.datacamp.com/es/tutorial/fine-tuning-llama-2" target="_blank">Spanish version</a> üá™üá∏ of this article.</td> 
        </tr> 
       </tbody> 
      </table> 
     </div> 
     <p dir="ltr">After the launch of the first version of <a rel="noopener" href="https://www.datacamp.com/blog/introduction-to-meta-ai-llama" target="_blank">LLaMA</a> by Meta, there was a new arms race to build better Large Language Models (LLMs) that could rival models like GPT-3.5 (ChatGPT). The open-source community rapidly released increasingly powerful models. It felt like Christmas for AI enthusiasts, with new developments announced frequently.</p> 
     <p dir="ltr">However, these advances came with downsides. Most open-source models carry restricted licensing, meaning they can only be used for research purposes. Secondly, only large companies or research institutes with sizable budgets could afford to fine-tune or train the models. Lastly, deploying and maintaining state-of-the-art large models was expensive.</p> 
     <p dir="ltr">The new version of LLaMA models aims to address these issues. It features a commercial license, making it accessible to more organizations. Additionally, new methodologies now allow fine-tuning on consumer GPUs with limited memory.</p> 
     <p dir="ltr">This democratization of AI is critical for widespread adoption. By overcoming barriers to entry, even small companies can build customized models suited to their needs and budgets.</p> 
     <p dir="ltr">In this tutorial, we will explore Llama-2 and demonstrate how to fine-tune it on a new dataset using Google Colab. Additionally, we will cover new methodologies and fine-tuning techniques that can help reduce memory usage and speed up the training process.</p> 
     <p dir="ltr"><em><img src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697724450%2FFine_Tune_L_La_MA_2_cc6aa0e4ad.png&amp;valid=true" loading="lazy"></em></p> 
     <p dir="ltr"><em>Image generated by Author using DALL-E 3</em></p> 
     <h2 dir="ltr">Understanding Llama 2 and Model Fine-Tuning</h2> 
     <p dir="ltr">Llama 2 is a collection of second-generation open-source LLMs from Meta that comes with a commercial license. It is designed to handle a wide range of natural language processing tasks, with models ranging in scale from 7 billion to 70 billion parameters. Discover more about LLaMA models by reading our article, <a rel="noopener" href="https://www.datacamp.com/blog/introduction-to-meta-ai-llama" target="_blank">Introduction to Meta AI's LLaMA: Empowering AI Innovation</a>.</p> 
     <p dir="ltr">Llama-2-Chat, which is optimized for dialogue, has shown similar performance to popular closed-source models like ChatGPT and PaLM. We can even improve the performance of the model by fine-tuning it on a high-quality conversational dataset.</p> 
     <p dir="ltr">Fine-tuning in machine learning is the process of adjusting the weights and parameters of a pre-trained model on new data to improve its performance on a specific task. It involves training the model on a new dataset that is specific to the task at hand while updating the model's weights to adapt to the new data. Read more about fine-tuning by following our <a rel="noopener" href="https://www.datacamp.com/tutorial/how-to-fine-tune-gpt3-5" target="_blank">guide to fine-tuning GPT 3.5</a>.</p> 
     <p dir="ltr">It is impossible to fine-tune LLMs on consumer hardware due to inadequate VRAMs and computing. However, in this tutorial, we will overcome these memory and computing challenges and train our model using a free version of Google Colab Notebook.</p> 
     <h2 dir="ltr">How to Fine-Tune Llama 2: A Step-By-Step Guide</h2> 
     <p dir="ltr">In this part, we will learn about all the steps required to fine-tune the Llama 2 model with 7 billion parameters on a T4 GPU. You have the option to use a free GPU on Google Colab or Kaggle. The code runs on both platforms.</p> 
     <p dir="ltr">The Colab T4 GPU has a limited 16 GB of VRAM. That is barely enough to store Llama 2‚Äì7b's weights, which means full fine-tuning is not possible, and we need to use parameter-efficient fine-tuning techniques like LoRA or QLoRA.</p> 
     <p dir="ltr">We will use the QLoRA technique to fine-tune the model in 4-bit precision and optimize VRAM usage. For that, we will use the Hugging Face ecosystem of LLM libraries: <code>transformers</code>, <code>accelerate</code>, <code>peft</code>, <code>trl</code>, and <code>bitsandbytes</code>.</p> 
     <h3 dir="ltr">1. Getting started</h3> 
     <p dir="ltr">We will start by installing the required libraries.</p> 
     <pre><p><code><span>%</span><span>%</span>capture
<span>%</span>pip install accelerate peft bitsandbytes transformers trl</code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <p dir="ltr">After that, we will load the necessary modules from these libraries.</p> 
     <pre><p><code><span>import</span> os
<span>import</span> torch
<span>from</span> datasets <span>import</span> load_dataset
<span>from</span> transformers <span>import</span> <span>(</span>
    AutoModelForCausalLM<span>,</span>
    AutoTokenizer<span>,</span>
    BitsAndBytesConfig<span>,</span>
    TrainingArguments<span>,</span>
    pipeline<span>,</span>
    logging<span>,</span>
<span>)</span>
<span>from</span> peft <span>import</span> LoraConfig
<span>from</span> trl <span>import</span> SFTTrainer</code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">2. Model configuration</h3> 
     <p>You can access the Meta‚Äôs official Llama-2 model from Hugging Face, but you have to apply for a request and wait a couple of days to get confirmation. Instead of waiting, we will use NousResearch‚Äôs <a rel="noopener" href="https://huggingface.co/NousResearch/Llama-2-7b-chat-hf" target="_blank">Llama-2-7b-chat-hf</a> as our base model. It is the same as the original but easily accessible.</p> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697712967%2Fimage1_8a929a5a46.png&amp;valid=true"></p> 
     <p dir="ltr"><em>Image from <a rel="noopener" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" target="_blank">Hugging Face</a></em></p> 
     <p dir="ltr">We will fine-tune our base model using a smaller dataset called <a rel="noopener" href="https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k" target="_blank">mlabonne/guanaco-llama2-1k</a> and write the name for the fine-tuned model.</p> 
     <pre><p><code><span># Model from Hugging Face hub</span>
base_model <span>=</span> <span>"NousResearch/Llama-2-7b-chat-hf"</span>

<span># New instruction dataset</span>
guanaco_dataset <span>=</span> <span>"mlabonne/guanaco-llama2-1k"</span>

<span># Fine-tuned model</span>
new_model <span>=</span> <span>"llama-2-7b-chat-guanaco"</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697713018%2Fimage3_31fc511133.png&amp;valid=true"></p> 
     <p dir="ltr"><em>Dataset at <a rel="noopener" href="https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k" target="_blank">Hugging Face</a></em></p> 
     <h3 dir="ltr">3. Loading dataset, model, and tokenizer</h3> 
     <p>We will load the ‚Äúguanaco-llama2-1k‚Äù dataset from the Hugging Face hub. The dataset contains 1000 samples and has been processed to match the Llama 2 prompt format, and is a subset of the excellent <a rel="noopener" href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco" target="_blank">timdettmers/openassistant-guanaco</a> dataset.</p> 
     <pre><p><code>dataset <span>=</span> load_dataset<span>(</span>guanaco_dataset<span>,</span> split<span>=</span><span>"train"</span><span>)</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <pre><p><code>Dataset parquet downloaded <span>and</span> prepared to <span>/</span>root<span>/</span><span>.</span>cache<span>/</span>huggingface<span>/</span>datasets<span>/</span>parquet<span>/</span>mlabonne<span>-</span><span>-</span>guanaco<span>-</span>llama2<span>-</span>1k<span>-</span>f1f1134768f90029<span>/</span><span>0.0</span><span>.0</span><span>/</span>0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901<span>.</span> Subsequent calls will reuse this data<span>.</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">4. 4-bit quantization configuration</h3> 
     <p dir="ltr">4-bit quantization via QLoRA allows efficient finetuning of huge LLM models on consumer hardware while retaining high performance. This dramatically improves accessibility and usability for real-world applications.</p> 
     <p dir="ltr">QLoRA quantizes a pre-trained language model to 4 bits and freezes the parameters. A small number of trainable Low-Rank Adapter layers are then added to the model.</p> 
     <p dir="ltr">During fine-tuning, gradients are backpropagated through the frozen 4-bit quantized model into only the Low-Rank Adapter layers. So, the entire pretrained model remains fixed at 4 bits while only the adapters are updated. Also, the 4-bit quantization does not hurt model performance.</p> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697713094%2Fimage7_3e12912d0d.png&amp;valid=true"></p> 
     <p dir="ltr"><em>Image from QLoRA <a rel="noopener" href="https://arxiv.org/abs/2305.14314" target="_blank">paper</a></em></p> 
     <p dir="ltr">You can read the <a rel="noopener" href="https://arxiv.org/abs/2305.14314" target="_blank">paper</a> to understand it better.</p> 
     <p dir="ltr">In our case, we create 4-bit quantization with NF4 type configuration using BitsAndBytes.</p> 
     <pre><p><code>compute_dtype <span>=</span> <span>getattr</span><span>(</span>torch<span>,</span> <span>"float16"</span><span>)</span>

quant_config <span>=</span> BitsAndBytesConfig<span>(</span>
    load_in_4bit<span>=</span><span>True</span><span>,</span>
    bnb_4bit_quant_type<span>=</span><span>"nf4"</span><span>,</span>
    bnb_4bit_compute_dtype<span>=</span>compute_dtype<span>,</span>
    bnb_4bit_use_double_quant<span>=</span><span>False</span><span>,</span>
<span>)</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">5. Loading Llama 2 model</h3> 
     <p dir="ltr">We will now load a model using 4-bit precision with the compute dtype "float16" from Hugging Face for faster training.</p> 
     <pre><p><code>model <span>=</span> AutoModelForCausalLM<span>.</span>from_pretrained<span>(</span>
    base_model<span>,</span>
    quantization_config<span>=</span>quant_config<span>,</span>
    device_map<span>=</span><span>{</span><span>""</span><span>:</span> <span>0</span><span>}</span>
<span>)</span>
model<span>.</span>config<span>.</span>use_cache <span>=</span> <span>False</span>
model<span>.</span>config<span>.</span>pretraining_tp <span>=</span> <span>1</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">6. Loading tokenizer</h3> 
     <p dir="ltr">Next, we will load the tokenizer from Hugginface and set <code>padding_side</code> to ‚Äúright‚Äù to fix the issue with fp16.</p> 
     <pre><p><code>tokenizer <span>=</span> AutoTokenizer<span>.</span>from_pretrained<span>(</span>base_model<span>,</span> trust_remote_code<span>=</span><span>True</span><span>)</span>
tokenizer<span>.</span>pad_token <span>=</span> tokenizer<span>.</span>eos_token
tokenizer<span>.</span>padding_side <span>=</span> <span>"right"</span></code></p></pre> 
     <h3 dir="ltr">7. PEFT parameters</h3> 
     <p dir="ltr">Traditional fine-tuning of pre-trained language models (PLMs) requires updating all of the model's parameters, which is computationally expensive and requires massive amounts of data.</p> 
     <p>Parameter-Efficient Fine-Tuning (PEFT) works by only updating a small subset of the model's most influential parameters, making it much more efficient. Learn about parameters by reading the PEFT official <a rel="noopener" href="https://huggingface.co/docs/peft/conceptual_guides/lora" target="_blank">documentation</a>.</p> 
     <pre><p><code>peft_params <span>=</span> LoraConfig<span>(</span>
    lora_alpha<span>=</span><span>16</span><span>,</span>
    lora_dropout<span>=</span><span>0.1</span><span>,</span>
    r<span>=</span><span>64</span><span>,</span>
    bias<span>=</span><span>"none"</span><span>,</span>
    task_type<span>=</span><span>"CAUSAL_LM"</span><span>,</span>
<span>)</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">8. Training parameters</h3> 
     <p dir="ltr">Below is a list of hyperparameters that can be used to optimize the training process:</p> 
     <ul> 
      <li><strong>output_dir: </strong>The output directory is where the model predictions and checkpoints will be stored.</li> 
      <li><strong>num_train_epochs:</strong> One training epoch.</li> 
      <li><strong>fp16/bf16: </strong>Disable fp16/bf16 training.</li> 
      <li><strong>per_device_train_batch_size:</strong> Batch size per GPU for training.</li> 
      <li><strong>per_device_eval_batch_size:</strong> Batch size per GPU for evaluation.</li> 
      <li><strong>gradient_accumulation_steps:</strong> This refers to the number of steps required to accumulate the gradients during the update process.</li> 
      <li><strong>gradient_checkpointing:</strong> Enabling gradient checkpointing.</li> 
      <li><strong>max_grad_norm:</strong> Gradient clipping.</li> 
      <li><strong>learning_rate:</strong> Initial learning rate.</li> 
      <li><strong>weight_decay: </strong>Weight decay is applied to all layers except bias/LayerNorm weights.</li> 
      <li><strong>Optim:</strong> Model optimizer (AdamW optimizer).</li> 
      <li><strong>lr_scheduler_type:</strong> Learning rate schedule.</li> 
      <li><strong>max_steps:</strong> Number of training steps.</li> 
      <li><strong>warmup_ratio:</strong> Ratio of steps for a linear warmup.</li> 
      <li><strong>group_by_length:</strong> This can significantly improve performance and accelerate the training process.</li> 
      <li><strong>save_steps:</strong> Save checkpoint every 25 update steps.</li> 
      <li><strong>logging_steps: </strong><span>Log every 25 update steps.</span></li> 
     </ul> 
     <pre><p><code>training_params <span>=</span> TrainingArguments<span>(</span>
    output_dir<span>=</span><span>"./results"</span><span>,</span>
    num_train_epochs<span>=</span><span>1</span><span>,</span>
    per_device_train_batch_size<span>=</span><span>4</span><span>,</span>
    gradient_accumulation_steps<span>=</span><span>1</span><span>,</span>
    optim<span>=</span><span>"paged_adamw_32bit"</span><span>,</span>
    save_steps<span>=</span><span>25</span><span>,</span>
    logging_steps<span>=</span><span>25</span><span>,</span>
    learning_rate<span>=</span><span>2e-4</span><span>,</span>
    weight_decay<span>=</span><span>0.001</span><span>,</span>
    fp16<span>=</span><span>False</span><span>,</span>
    bf16<span>=</span><span>False</span><span>,</span>
    max_grad_norm<span>=</span><span>0.3</span><span>,</span>
    max_steps<span>=</span><span>-</span><span>1</span><span>,</span>
    warmup_ratio<span>=</span><span>0.03</span><span>,</span>
    group_by_length<span>=</span><span>True</span><span>,</span>
    lr_scheduler_type<span>=</span><span>"constant"</span><span>,</span>
    report_to<span>=</span><span>"tensorboard"</span>
<span>)</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <h3 dir="ltr">9. Model fine-tuning</h3> 
     <p dir="ltr"><strong>Supervised fine-tuning</strong> (SFT) is a key step in reinforcement learning from human feedback (RLHF). The TRL library from HuggingFace provides an easy-to-use API to create SFT models and train them on your dataset with just a few lines of code. It comes with tools to train language models using reinforcement learning, starting with supervised fine-tuning, then reward modeling, and finally, proximal policy optimization (PPO).</p> 
     <p dir="ltr">We will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.</p> 
     <pre><p><code>trainer <span>=</span> SFTTrainer<span>(</span>
    model<span>=</span>model<span>,</span>
    train_dataset<span>=</span>dataset<span>,</span>
    peft_config<span>=</span>peft_params<span>,</span>
    dataset_text_field<span>=</span><span>"text"</span><span>,</span>
    max_seq_length<span>=</span><span>None</span><span>,</span>
    tokenizer<span>=</span>tokenizer<span>,</span>
    args<span>=</span>training_params<span>,</span>
    packing<span>=</span><span>False</span><span>,</span>
<span>)</span></code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <p>We will use <code>.train()</code> to fine-tune the Llama 2 model on a new dataset. It took one and a half hours for the model to complete 1 epoch.</p> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697713295%2Fimage2_bd07e2c7be.png&amp;valid=true"></p> 
     <p dir="ltr">After training the model, we will save the model adopter and tokenizers. You can also upload the model to Hugging Face using a similar API.</p> 
     <pre><p><code>trainer<span>.</span>model<span>.</span>save_pretrained<span>(</span>new_model<span>)</span>
trainer<span>.</span>tokenizer<span>.</span>save_pretrained<span>(</span>new_model<span>)</span>
</code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697713331%2Fimage4_c1b28c099f.png&amp;valid=true"></p> 
     <h3 dir="ltr">10. Evaluation</h3> 
     <p dir="ltr">We can now review the training results in the interactive session of Tensorboard.</p> 
     <pre><p><code><span>from</span> tensorboard <span>import</span> notebook
log_dir <span>=</span> <span>"results/runs"</span>
notebook<span>.</span>start<span>(</span><span>"--logdir {} --port 4000"</span><span>.</span><span>format</span><span>(</span>log_dir<span>)</span><span>)</span>
</code></p>
  
      <div>
   
       <p> OpenAI</p>
  
      </div><span data-track></span><p><span>Was this helpful?</span></p></pre> 
     <p><img loading="lazy" src="https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimages.datacamp.com%2Fimage%2Fupload%2Fv1697713367%2Fimage5_47e605c3c8.png&amp;valid=true"></p> 
     <p dir="ltr">To test our fine-tuned model, we will use <code>transformers</code> text generation pipeline and ask simple questions like ‚ÄúWho is Leonardo Da Vinci?‚Äù.</p> 
     <pre><p><code>logging<span>.</span>set_verbosity<span>(</span>logging<span>.</span>CRITICAL<span>)</span>

prompt <span>=</span> <span>"Who is Leonardo Da Vinci?"</span>
pipe <span>=</span> pipeline<span>(</span>task<span>=</span><span>"text-generation"</span><span>,</span> model<span>=</span>model<span>,</span> tokenizer<span>=</span>tokenizer<span>,</span> max_length<span>=</span><span>200</span><span>)</span>
result <span>=</span> pipe<span>(</span><span><span>f"&lt;s&gt;[INST] </span><span><span>{</span>prompt<span>}</span></span><span> [/INST]"</span></span><span>)</span>
<span>print</span><span>(</span>result<span>[</span><span>0</span><span>]</span><span>[</span><span>'generated_text'</span><span>]</span><span>)</span></code></p></pre> 
     <p dir="ltr"><strong>Output:</strong></p> 
     <p dir="ltr">As we can see, we got amazing results.</p> 
     <pre><p><code><span>&lt;</span>s<span>&gt;</span><span>[</span>INST<span>]</span> Who <span>is</span> Leonardo Da Vinci? <span>[</span><span>/</span>INST<span>]</span> Leonardo da Vinci <span>(</span><span>1452</span><span>-</span><span>1519</span><span>)</span> was an Italian polymath<span>,</span> artist<span>,</span> inventor<span>,</span> <span>and</span> engineer<span>.</span>

Da Vinci <span>is</span> widely considered one of the greatest painters of <span>all</span> time<span>,</span> <span>and</span> his works include the famous Mona Lisa<span>.</span> He was also an accomplished engineer<span>,</span> inventor<span>,</span> <span>and</span> anatomist<span>,</span> <span>and</span> his designs <span>for</span> machines <span>and</span> flight were centuries ahead of his time<span>.</span>

Da Vinci was born <span>in</span> the town of Vinci<span>,</span> Italy<span>,</span> <span>and</span> he was the illegitimate son of a local notary<span>.</span> Despite his humble origins<span>,</span> he was able to study art <span>and</span> engineering <span>in</span> Florence<span>,</span> <span>and</span> he became a renowned artist <span>and</span> inventor<span>.</span>

Da Vinci's work had a profound impact on the Renaissance<span>,</span> <span>and</span> his legacy continues to inspire artists<span>,</span> engineers<span>,</span> <span>and</span> inventors to this day<span>.</span> He
</code></p></pre> 
     <p dir="ltr">Let‚Äôs ask another question.</p> 
     <pre><p><code>prompt <span>=</span> <span>"What is Datacamp Career track?"</span>
result <span>=</span> pipe<span>(</span><span><span>f"&lt;s&gt;[INST] </span><span><span>{</span>prompt<span>}</span></span><span> [/INST]"</span></span><span>)</span>
<span>print</span><span>(</span>result<span>[</span><span>0</span><span>]</span><span>[</span><span>'generated_text'</span><span>]</span><span>)</span></code></p></pre> 
     <p dir="ltr"><strong>Output:</strong></p> 
     <p dir="ltr">Guanaco is a high-quality dataset that has been used to fine-tune state-of-the-art LLMs in the past. The entire Guanaco dataset is available on Hugging Face and it has the potential to achieve even greater performance on a variety of natural language tasks.</p> 
     <pre><p><code><span>&lt;</span>s<span>&gt;</span><span>[</span>INST<span>]</span> What <span>is</span> Datacamp Career track? <span>[</span><span>/</span>INST<span>]</span> DataCamp Career Track <span>is</span> a program that offers a comprehensive learning experience to <span>help</span> you build your skills <span>and</span> prepare <span>for</span> a career <span>in</span> data science<span>.</span>

The program includes a <span>range</span> of courses<span>,</span> projects<span>,</span> <span>and</span> assessments that are designed to <span>help</span> you build your skills <span>in</span> data science<span>.</span> You will learn how to work <span>with</span> data<span>,</span> create visualizations<span>,</span> <span>and</span> build predictive models<span>.</span>

In addition to the technical skills<span>,</span> you will also learn how to communicate your findings to stakeholders <span>and</span> how to work <span>with</span> a team to solve <span>complex</span> problems<span>.</span>

The program <span>is</span> designed to be flexible<span>,</span> so you can learn at your own pace <span>and</span> on your own schedule<span>.</span> You will also have access to a community of learners <span>and</span> mentors who can provide support <span>and</span> guidance throughout the program<span>.</span>

Overall<span>,</span> DataCamp Career Track <span>is</span> a great way to build your skills <span>and</span> prepare <span>for</span> a career <span>in</span></code></p></pre> 
     <p dir="ltr">Here is the <a rel="noopener" href="https://colab.research.google.com/drive/1tG9eqttfnqHoQqmsiacywUG9ilUhoiCk?usp=sharing" target="_blank">Colab Notebook</a> with the code and the outputs to assist you in your coding journey.</p> 
     <p dir="ltr">Next, you can use LlamaIndex and build your own AI application using your new training model by following the <a rel="noopener" href="https://www.datacamp.com/tutorial/llama-index-adding-personal-data-to-llms" target="_blank">LlamaIndex: Adding Personal Data to LLMs</a> tutorial. You can get inspiration for your project by checking out <a rel="noopener" href="https://www.datacamp.com/blog/5-projects-you-can-build-with-generative-ai-models" target="_blank">5 Projects Built with Generative Models and Open Source Tools</a>.</p> 
     <h2 dir="ltr">Conclusion</h2> 
     <p dir="ltr">The tutorial provided a comprehensive guide on fine-tuning the LLaMA 2 model using techniques like QLoRA, PEFT, and SFT to overcome memory and compute limitations. By leveraging Hugging Face libraries like <code>transformers</code>, <code>accelerate</code>, <code>peft</code>, <code>trl</code>, and <code>bitsandbytes</code>, we were able to successfully fine-tune the 7B parameter LLaMA 2 model on a consumer GPU.</p> 
     <p dir="ltr">Overall, this tutorial exemplified how recent advances have enabled the democratization and accessibility of large language models, allowing even hobbyists to build state-of-the-art AI with limited resources.</p> 
     <p dir="ltr">If you are new to large language models, consider taking the <a rel="noopener" href="https://www.datacamp.com/courses/large-language-models-llms-concepts" target="_blank">Master LLMs Concepts</a> course. And if you want to start your career in artificial intelligence, then you should enroll in the <a rel="noopener" href="https://www.datacamp.com/tracks/ai-fundamentals" target="_blank">AI Fundamentals</a> skill track.</p> 
     <h2 dir="ltr">Reference</h2> 
     <ul> 
      <li><a rel="noopener" href="https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32" target="_blank">Fine-Tune Your Own Llama 2 Model in a Colab Notebook</a></li> 
      <li><a rel="noopener" href="https://huggingface.co/docs/trl/main/en/sft_trainer" target="_blank">Supervised Fine-tuning Trainer</a></li> 
      <li><a rel="noopener" href="https://huggingface.co/docs/trl/main/en/index" target="_blank">TRL - Transformer Reinforcement Learning</a></li> 
      <li><a rel="noopener" href="https://huggingface.co/docs/peft/index" target="_blank">PEFT</a></li> 
      <li><a rel="noopener" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" target="_blank">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li> 
     </ul> 
     <p></p> 
    </div>
    <p class="reader-footer"><a class="reader-footer-source" href="https://cubox.pro/my/card?id=7178036008582120133" target="_blank"><span class="reader-footer-source-label">Ë∑≥ËΩ¨Âà∞ Cubox Êü•Áúã</span></a></p>
   </div>
  </div>
  <script type="text/javascript" src="https://cubox.pro/article/js/reader.js"></script>
 </body>
</html>